{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 機械学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/machine_learning.png\" width=\"800\"></img>  \n",
    "引用元: [AINOW[初心者でもわかるディープラーニングー基礎知識からAIとの違い、導入プロセスまで細かく解説]](https://ainow.ai/2019/08/06/174245/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI**（Artificial Intelligence、人工知能） = 人間の知的な行動を模倣するコンピュータシステム全般を指す**広い概念**  \n",
    "人間のように**思考し、学習し、判断し、解決できるシステム**（例えば選択式のチャットボットとか）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 機械学習とは\n",
    "- コンピュータが**データ**から学び、新しい情報や問題に対応する技術や方法\n",
    "- **データ**から自動的にパターンを「**学習**」し、予測や意思決定を行うことができる\n",
    "\n",
    "例）お掃除ロボット、チャットボット、店舗来客分析、生産量予測などなど\n",
    "\n",
    "<img src='img/chatbot_ai.png'></img>  \n",
    "引用元: [qualva[チャットボットとは？種類・仕組みごとの特徴、導入する目的を徹底解説！]](https://qualva.com/qualvatics/archive/477/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主な種類\n",
    "- <b>教師あり学習</b>：入力データに対する正解データを用いてモデルを訓練し、新しいデータに予測を行う\n",
    "- 教師無し学習：データ内の傾向やパターンを見つけ出す\n",
    "- 強化学習：エージェント（AI）に試行錯誤を行わせながら、最適な挙動をするよう学習させる\n",
    "\n",
    "今回行うのは教師あり学習！！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 教師あり学習\n",
    "例えば、画像から犬と猫を分類するAI  \n",
    "<img src='img/ai_image.png' width=\"800\"></img>  \n",
    "犬または猫の画像をAIに入れると、それが犬なのか猫なのか応えてくれる。  \n",
    "<img src='img/ai_image_2.png' width=\"800\"></img>  \n",
    "これを式で表現すると$x$を$f(x)$に入力して、$y$を出力するということになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで関数$f$を**モデル**という。  \n",
    "モデルには様々な形があるので、適切な形を選ぶ必要がある。 \n",
    "例えば、  \n",
    "$$f(x)=ax^3+bx^2+cx+d$$  \n",
    "ここで$a, b, c, d$を **重み（パラメータ）** という。（一般的には $w$ (weight)で表す）  \n",
    "この重みを適切に（入力データに対して正しい正解データを出力するように）変えていくことが学習です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深層学習（ディープラーニング）とは\n",
    "<img src=\"img/neural_net.png\" width=\"800\"></img>  \n",
    "引用元: [翔泳社[ディープラーニングと脳の関係とは？ 人工ニューロンや再帰型ニューラルネットワークを解説]](https://www.shoeisha.co.jp/book/article/detail/304)\n",
    "\n",
    "- 多層の「**ニューラルネットワーク**」を使い、**複雑な特徴**を学習する手法\n",
    "- 従来の手法では困難だった高度なタスク（画像認識、音声認識、自然言語処理など）を高精度で実現  \n",
    "\n",
    "例）畳み込みニューラルネットワーク（CNN）、リカレントニューラルネットワーク（RNN）など"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 機械学習の主な手順\n",
    "1. <b>データの収集・整理</b>\n",
    "    - 数値、テキスト、画像、音声など、タスクに応じて\n",
    "    - 十分な質と量が必要で、一般的にデータが多いほど精度を向上させられる\n",
    "\n",
    "2. <b>前処理</b>\n",
    "    - モデルが学習しやすいように、データを変換したり、特徴量を抽出したり\n",
    "\n",
    "3. <b>モデル・評価基準の設定</b>\n",
    "    - モデル＝未知のデータセットからパターンを発見したり、予測のためのルールを構築するプログラム\n",
    "    - 評価基準＝モデルがどの程度の精度で予測できているかを定量的に計測するもの（損失関数・評価関数）\n",
    "\n",
    "4. <b>学習</b>\n",
    "    - 問題に適したアルゴリズムで、訓練用データを使ってモデルを訓練\n",
    "    - 評価基準をもとに、より良い精度を実現しようと学習する\n",
    "\n",
    "5. <b>結果の可視化</b>\n",
    "    - 評価指標を算出し、グラフにプロット\n",
    "    - 予測結果を出力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本日のタスク"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セマンティックセグメンテーション（Semantic Segmentation）\n",
    "- 画像内のピクセルごとに「何が映っているか」を識別し、それぞれを特定のカテゴリに分類する技術\n",
    "- 医療現場や自動運転技術など、様々な場所で使われている  \n",
    "\n",
    "<img src=\"img/semantic_segmentation.png\" width=\"800\"></img>  \n",
    "引用元: [DAGS[Scene Understanding Datasets]](http://dags.stanford.edu/projects/scenedataset.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この場合、  \n",
    "入力 $x=様々なものが映った画像$  \n",
    "出力 $y=ラベル毎に該当の色で塗りつぶされたマスク画像$  \n",
    "具体例は後で"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用するモデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNet\n",
    "- セグメンテーションに使われるモデル  \n",
    "- 画像生成AIにもよく使われる  \n",
    "\n",
    "<img src=\"img/unet.png\" width=\"800\"></img><br>\n",
    "引用元: [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実際に学習を体験"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先ほどの手順に沿って行ってみます。\n",
    "><b>機械学習の主な手順</b>\n",
    ">1. データの収集・整理\n",
    ">2. 前処理\n",
    ">3. モデル・評価基準の設定\n",
    ">4. 学習\n",
    ">5. 結果の可視化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリのインポート\n",
    "ライブラリ：Pythonで開発を行うにあたって良く使われる関数やパッケージがまとめられたもの（必要な道具）  \n",
    "\n",
    "[標準ライブラリ]\n",
    "- `os`: OSに依存しているさまざまな機能を利用するためのモジュール（例：ファイル作成やパスの確認）\n",
    "- `math`: 数学関係\n",
    "\n",
    "[外部ライブラリ]\n",
    "- `numpy`: 数値計算\n",
    "- `PIL`: 画像処理\n",
    "- `pandas`: データフレームという表のようなものを扱う\n",
    "- `matplotlib`: グラフ等可視化\n",
    "- `tqdm`: プログレスバーを出せる（こんなやつ⇒`100%|██████████| 100/100 [01:40<00:00,  1.00s/it]`）\n",
    "- `torch`: 機械学習関係\n",
    "- `torchvision`: 画像機械学習関連\n",
    "\n",
    "[自作モジュール・ローカルモジュール]\n",
    "- `unet`: Unet\n",
    "- `dataset`: データセット\n",
    "- `utils`: その他使用する関数等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from unet import UNet\n",
    "from loss import DiceCrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. データの収集・整理 + 2. 前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 今回使用するデータ\n",
    "[Visual Object Classes Challenge 2012 (VOC2012)](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html#devkit)  \n",
    "- 検出対象は20種類で、「背景」と「未分類」合わせると22クラス\n",
    "- 訓練データが1464枚、テストデータが1449枚\n",
    "\n",
    ">@misc{pascal-voc-2012,\n",
    ">\tauthor = \"Everingham, M. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.\",\n",
    ">\ttitle = \"The {PASCAL} {V}isual {O}bject {C}lasses {C}hallenge 2012 {(VOC2012)} {R}esults\",\n",
    ">\thowpublished = \"http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画像サンプル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力画像・正解画像の例\n",
    "input = Image.open('VOCdevkit/VOC2012_sample/JPEGImages/2007_000032.jpg')\n",
    "gt = Image.open('VOCdevkit/VOC2012_sample/SegmentationClass/2007_000032.png')\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "fig.add_subplot(2,3,1).set_title('input')\n",
    "plt.imshow(input)\n",
    "fig.add_subplot(2,3,2).set_title('gt')\n",
    "plt.imshow(gt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### カラーパレット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC2012で用いるラベル\n",
    "CLASSES = ['backgrounds','aeroplane','bicycle','bird','boat','bottle',\n",
    "            'bus','car' ,'cat','chair','cow', \n",
    "            'diningtable','dog','horse','motorbike','person', \n",
    "            'potted plant', 'sheep', 'sofa', 'train', 'monitor','unlabeld'\n",
    "            ]\n",
    "\n",
    "# カラーパレットの作成\n",
    "COLOR_PALETTE = np.array(Image.open(\"./VOCdevkit/VOC2012_sample/SegmentationClass/2007_000170.png\").getpalette()).reshape(-1,3)\n",
    "COLOR_PALETTE = COLOR_PALETTE.tolist()[:len(CLASSES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カラーパレットの可視化\n",
    "fig, axes_list = plt.subplots(len(CLASSES), 1, figsize=(5, 10))\n",
    "for i, color in enumerate(COLOR_PALETTE[:len(CLASSES)]):\n",
    "    color_img = np.full((1, 10, 3), color, dtype=np.uint8)\n",
    "\n",
    "    axes_list[i].imshow(color_img, aspect='auto')\n",
    "    axes_list[i].set_axis_off()\n",
    "    axes_list[i].text(-1, 0, f'{i}: {CLASSES[i]}', va='center', ha='right', fontsize=10)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回はデモなので訓練データ300枚、テストデータ100枚で学習を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データのリストファイル\n",
    "train_list_path = 'VOCdevkit/VOC2012_sample/listfile/train_list_300.txt'\n",
    "val_list_path = 'VOCdevkit/VOC2012_sample/listfile/val_list_100.txt'\n",
    "\n",
    "# データディレクトリ\n",
    "img_dir = 'VOCdevkit/VOC2012_sample/JPEGImages'\n",
    "gt_dir = 'VOCdevkit/VOC2012_sample/SegmentationClass'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# リストファイルの読み込み\n",
    "with open(train_list_path, 'r') as f:\n",
    "    train_list = f.read().splitlines()\n",
    "with open(val_list_path, 'r') as g:\n",
    "    val_list = g.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセット\n",
    "入力データと正解データをペアで保持するもの  \n",
    "前処理もここで行うことが多い（Tensorに変換、リサイズ、データ拡張など）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずはデータセットクラスの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DataTransforme(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    データの前処理をするクラス\n",
    "    tensor型に変換 -> リサイズ\n",
    "    Args:\n",
    "    input_size: int, リサイズ先の画像の大きさ\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.resize = transforms.Resize(size=(input_size, input_size), interpolation=transforms.InterpolationMode.NEAREST)\n",
    "\n",
    "    def forward(self, img, gt):\n",
    "        img_tensor = self.to_tensor(img)\n",
    "        img_resized = self.resize(img_tensor)\n",
    "\n",
    "        gt_np = np.asarray(gt)\n",
    "        gt_np = np.where(gt_np == 255, 0, gt_np)\n",
    "        gt_tensor = torch.tensor(gt_np, dtype=torch.long)\n",
    "        gt_tensor = gt_tensor.unsqueeze(0)\n",
    "        gt_resized = self.resize(gt_tensor)\n",
    "\n",
    "        return img_resized, gt_resized\n",
    "    \n",
    "class VOCDataset(Dataset):\n",
    "    \"\"\"\n",
    "    VOC2012のDatasetクラス\n",
    "    Args:\n",
    "    img_list: list, 画像のファイル名のリスト\n",
    "    img_dir: str, 画像のディレクトリ\n",
    "    gt_dir: str, 正解画像のディレクトリ\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            img_list: list, \n",
    "            img_dir: str,\n",
    "            gt_dir: str,\n",
    "    ):\n",
    "        self.img_list = img_list\n",
    "        self.img_dir = img_dir\n",
    "        self.gt_dir = gt_dir\n",
    "        self.preprocessing = DataTransforme(input_size=256)\n",
    "        self.img_fps = [os.path.join(img_dir, img_id)+\".jpg\" for img_id in self.img_list]\n",
    "        self.gt_fps = [os.path.join(gt_dir, gt_id)+\".png\" for gt_id in self.img_list]\n",
    "        self.CLASSES = CLASSES\n",
    "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in self.CLASSES]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        # 画像と正解画像の読み込み\n",
    "        img = Image.open(self.img_fps[i])\n",
    "        gt = Image.open(self.gt_fps[i])\n",
    "        \n",
    "        # 前処理\n",
    "        img, gt = self.preprocessing(img, gt)\n",
    "\n",
    "        return img, gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの作成\n",
    "train_ds = VOCDataset(img_list=train_list, img_dir=img_dir, gt_dir=gt_dir)\n",
    "val_ds = VOCDataset(img_list=val_list, img_dir=img_dir, gt_dir=gt_dir)\n",
    "print(f\"len(train_data): {train_ds.__len__()}\")\n",
    "print(f\"len(test_data): {val_ds.__len__()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データローダー\n",
    "データセットからサンプルを取得し、実際に取り出すもの\n",
    "\n",
    "[Args]  \n",
    "- `batch_size`: バッチサイズ、データの分割数（大きくするほど高速だが、メモリが必要）\n",
    "- `num_workers`: ワーカー数、動かすCPUのコア数（こちらも基本的には大きいほど速いが、逆効果の場合も）\n",
    "- `shuffle`: データのシャッフル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データローダーの作成\n",
    "train_dl = DataLoader(train_ds, batch_size=16, num_workers=4, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=16, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. モデル・評価基準の設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### デバイスの設定\n",
    "`cuda`はGPUのこと。（厳密には違うけど）  \n",
    "`Using cuda device`のように出力されればGPUが使えるということ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの設定\n",
    "今回はUNetを用います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet() # モデル\n",
    "model = model.to(device) # デバイスに送る"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 評価基準 （損失関数）\n",
    "文字通りそのモデルの現状の精度を測る指標  \n",
    "基本的には損失関数と呼ばれ、この損失（ロス）を下げる方向に学習（パラメータを調整していく）\n",
    "\n",
    "今回は以下の関数を使う  \n",
    "- Dice Cross-Entropy Loss（小さいほど高精度）\n",
    "- ダイス損失（Dice Loss）とクロスエントロピー損失を足し合わせた損失関数\n",
    "\n",
    "**ダイス損失（Dice Loss）**  \n",
    "$$Dice Loss = 1 - \\frac{2|A \\cap B|}{|A| + |B|}$$  \n",
    "<img src='img/dice_loss.png' width=800></img>  \n",
    "引用元：[Dice Loss In Medical Image Segmentation](https://cvinvolution.medium.com/dice-loss-in-medical-image-segmentation-d0e476eb486)\n",
    "\n",
    "**クロスエントロピー損失**  \n",
    "$$Cross Entropy Loss = -\\frac{1}{N}\\sum_{i=1}^Np(x)\\log{q(x)}$$  \n",
    "<img src='img/cross_entropy.png' width=800></img>  \n",
    "引用元：[自然言語処理：ニューラルネット、クロスエントロピー・ロス関数](https://www.youtube.com/watch?v=MussKg8FfEU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# クロスエントロピー損失のクラスごとの重み\n",
    "weight = None # torch.tensor([0.1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5]) \n",
    "\n",
    "# 損失関数のDice Lossの比率\n",
    "dice_weight = 0.5\n",
    "\n",
    "if weight is not None:\n",
    "    weight = weight.to(device)\n",
    "\n",
    "# 損失関数\n",
    "criterion =  DiceCrossEntropyLoss(weight=weight, dice_weight=dice_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最適化手法\n",
    "損失関数を最小化するために、モデルのパラメータを更新するアルゴリズム  \n",
    "例）確率的勾配降下法、モーメンタム、AdaGradなど\n",
    "\n",
    "今回は **Adam（Adaptive Moment Estimation）** というものを用いる。\n",
    ">**学習率**  \n",
    ">パラメータ（モデルの重みなど）をどれだけの大きさで更新するかを決める重要なハイパーパラメータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01 # 学習率\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr) # 最適化手法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練\n",
    "1. モデルに入力データを入れて推論\n",
    "2. 推論結果と正解データを損失関数を使って損失（ロス）を計算\n",
    "3. 誤差逆伝播（バックプロパゲーション）で損失と最適化手法をもとにパラメータを修正\n",
    "4. 次のデータを入力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    batch_size = dataloader.batch_size\n",
    "    step_total = math.ceil(size/batch_size)\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with tqdm.tqdm(enumerate(dataloader), total=step_total) as pbar:\n",
    "        for batch, item in pbar:\n",
    "            inp, gt = item[0].to(device), item[1].to(device)\n",
    "\n",
    "            # 推論\n",
    "            pred = model(inp)\n",
    "\n",
    "            # 損失誤差を計算\n",
    "            loss = criterion(pred, gt)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # バックプロパゲーション\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # プログレスバーに損失を表示\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_loss = total_loss / step_total\n",
    "        print(f\"Train Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テスト\n",
    "1. 入力データをモデルに入力\n",
    "2. 推論結果と正解データから損失を計算\n",
    "3. 全テストデータで1～2を繰り返す\n",
    "4. 平均のロスを計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, criterion, device):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    batch_size = dataloader.batch_size\n",
    "    step_total = math.ceil(size/batch_size)\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for item in dataloader:\n",
    "            inp, gt = item[0].to(device), item[1].to(device)\n",
    "            pred = model(inp)\n",
    "            loss = criterion(pred, gt)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / step_total\n",
    "        print(f\"Val Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### エポック\n",
    "訓練データを何回使ったかを表す数⇒何回訓練を行うか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epoch = 0 # ここは変えない\n",
    "epochs = 10 # エポック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存先の設定\n",
    "save_dir = './unet_demo' # 訓練ログ保存ディレクトリ\n",
    "save_name_prefix = 'unet_demo' # 訓練ログ保存名\n",
    "csv_path = os.path.join(save_dir, f'{save_name_prefix}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存ディレクトリの作成\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "for epoch in range(initial_epoch, epochs):\n",
    "    print(f\"Epoch {epoch}\\n-------------------------------\")\n",
    "    train_loss = train(train_dl, model, optimizer, criterion, device)\n",
    "    val_loss = test(val_dl, model, criterion, device)\n",
    "    # 訓練ログ記入\n",
    "    if not os.path.exists(csv_path):\n",
    "        with open(csv_path, 'w') as f:\n",
    "            f.write('epoch,train_loss,val_loss\\n')\n",
    "    with open(csv_path, 'a') as f:\n",
    "        f.write(f'{epoch},{train_loss},{val_loss}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでターミナルを開いて、`nvidia-smi`と打ち込んでみよう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 結果の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論結果の可視化\n",
    "def visualize(model, img_id, img_dir, gt_dir, criterion, device='cpu'):\n",
    "    img_path = os.path.join(img_dir, img_id) + \".jpg\"\n",
    "    gt_path = os.path.join(gt_dir, img_id) + \".png\"\n",
    "    class_values = [CLASSES.index(cls.lower()) for cls in CLASSES]\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 入力画像の前処理 (PIL画像 -> Tensor)\n",
    "        inp_tensor = transforms.functional.to_tensor(Image.open(img_path))\n",
    "        inp_resized = transforms.Resize(size=(256, 256), interpolation=transforms.InterpolationMode.NEAREST)(inp_tensor)\n",
    "        inp = inp_resized.unsqueeze(0)\n",
    "\n",
    "        # 正解画像の前処理 (PIL画像 -> NumPy -> Tensor)\n",
    "        gt_np = np.asarray(Image.open(gt_path))\n",
    "        gt_np = np.where(gt_np == 255, 0, gt_np)  # 範囲外の255を0に置換（または別のインデックスに）\n",
    "\n",
    "        gt_tensor = torch.tensor(gt_np, dtype=torch.long)\n",
    "        gt_tensor = gt_tensor.unsqueeze(0)\n",
    "        gt_resized = transforms.Resize(size=(256, 256), interpolation=transforms.InterpolationMode.NEAREST)(gt_tensor)\n",
    "\n",
    "        # 予測\n",
    "        pred = model(inp)\n",
    "        # loss = criterion(pred, gt_resized) # loss計算\n",
    "        # print(f'Loss: {loss.item()}')\n",
    "\n",
    "        inp_np = (inp_resized.numpy().transpose(1, 2, 0) * 255).astype(np.uint8)  # 表示用スケール\n",
    "        pred_np = torch.argmax(pred, dim=1).cpu().detach().numpy()[0]\n",
    "        pred_np = np.clip(pred_np, 0, len(COLOR_PALETTE) - 1).astype(np.int32)\n",
    "        gt_resized = gt_resized.squeeze(0).detach().numpy().astype(np.int32)\n",
    "\n",
    "        # カラーパレットの適用\n",
    "        img_gt = np.array([[COLOR_PALETTE[gt_resized[i, j]] for j in range(256)] for i in range(256)], dtype=np.uint8)\n",
    "        img_pred = np.array([[COLOR_PALETTE[pred_np[i, j]] for j in range(256)] for i in range(256)], dtype=np.uint8)\n",
    "\n",
    "    # プロット\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    for i, im in enumerate([inp_np, img_gt, img_pred]):\n",
    "        ax = fig.add_subplot(1, 3, i+1)\n",
    "        ax.imshow(im)\n",
    "        ax.set_title([\"Input\", \"Ground Truth\", \"Prediction\"][i])\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 損失のプロット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = pd.read_csv(csv_path)\n",
    "plt.plot(log_df['epoch'], log_df['train_loss'], label='trian_data')\n",
    "plt.plot(log_df['epoch'], log_df['val_loss'], label='test_data')\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "# plt.ylim(0,2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推論結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>train data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = '2007_000032'\n",
    "visualize(model, img_id, img_dir, gt_dir, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = '2007_000648'\n",
    "visualize(model, img_id, img_dir, gt_dir, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = '2007_001225'\n",
    "visualize(model, img_id, img_dir, gt_dir, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>test data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = '2007_000033'\n",
    "visualize(model, img_id, img_dir, gt_dir, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = '2007_001763'\n",
    "visualize(model, img_id, img_dir, gt_dir, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = '2007_003367'\n",
    "visualize(model, img_id, img_dir, gt_dir, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
